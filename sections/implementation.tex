\section{Implementation}

\subsection{Launcher} The main point of entry to the \acrshort{cli} build tool is the \emph{launcher} executable. Acting as the starting point for all invocations, it is implemented as a \lstinline|bash| script mainly wrapping the \emph{Docker} client. To provide a repeatable build environment, the launcher prefers to start a well-known container to run the given command in. If, however, the launcher is invoked as part of an already-running \acrshort{ci} container, it is not possible to start a new one. Thus, the launcher first has to detect whether it is able to start containers. In case it is able to, the launcher has to pull the correct image for the host's machine architecture in combination with the requested Erlang/OTP version, set up volume mounts and environment variables, and invoke the Docker client to start the container. If the launcher is already running inside a container, the responsibility is reduced to passing through the given arguments to the \emph{container entrypoint}. The following paragraphs describe how each component of the launch sequence is implemented.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}[>=latex']
    \tikzset{block/.style={
      draw,
      rectangle,
      align=center,
      minimum width=2.8cm,
      minimum height=1cm
    }};

    \node [block] (installer) {\textbf{Installer}};
    \node [block, right=1cm of installer] (launcher) {\textbf{Launcher}};
    \node [block, right=1cm of launcher] (entrypoint) {\textbf{Entrypoint}};
    \node [block, right=1cm of entrypoint] (command) {$<$\emph{Command}$>$};
    \node [block, below=0.8cm of launcher] (docker) {Docker};

    \node [inner sep=0pt,
      yshift=-1.8cm,
      minimum height=0.6cm,
      fit={(entrypoint) (command)},
      label=center:Linux Container] (container) {};

    \node [
      above left=1cm and -0.5cm of launcher,
      label=left:\text{``\lstinline|beamup| $<$\emph{Command}$>$}''] (start) {};

    \path[draw,->] (launcher) edge (entrypoint)
      (entrypoint) edge (command);

    \path[densely dashed,->] (installer) edge (launcher)
      (launcher) edge (docker);

    \path[draw,o->,thick] (start) edge[bend left] (launcher);

    \node [draw=black!50, densely dashed, fit={
      (entrypoint) (command) (container)
    }, inner sep=10pt] (container_box) {};

    \path[draw,->] (docker) edge (container -|container_box.west);

  \end{tikzpicture}
  \caption{Execution sequence of build pipeline components.}
\end{figure}

\paragraph{Installation.} The recommended way to install the \acrshort{cli} tool is by downloading a \lstinline|bash| script and piping it to the shell. This pattern is informally known as ``curl pipe sh'', and while common because of its simplicity, brings security implications which are discussed in section~\ref{sec:curlpipesh}. A Content Delivery Network, \emph{Cloudflare}, is used to ensure the installer script is exclusively served with \acrfull{tls}.

\begin{lstlisting}[
  label={lst:curlpipesh},
  caption={CLI tool installation command}
]
curl https://get.beamup.io/install | /bin/sh
\end{lstlisting}

When the installer detects it is being executed inside an interactive terminal session, it pauses for a few seconds to give the user a chance to abort installation. Next, the installer clones the \acrshort{cli} repository to the \lstinline|~/.beamup| folder of the current user's home directory. Note that cloning a repository keeps file permissions intact, including the executable bits that are set on the main \acrshort{cli} launcher script. At no point are super user permissions needed, consequently the installer first attempts to create a symbolic link to the \acrshort{cli} launcher, and optionally falls back to displaying instructions on how to add the just-installed tool to the executable path. Finally, to make sure that the installation was successful, the launcher is invoked for a self-test.

\paragraph{Self-update.} Because the \acrshort{cli} tool is little more than a local clone of a remote repository, distributing and installing updates to the tool itself becomes as trivial as pulling changes from the remote. A convenience wrapper is provided.

\paragraph{Project scaffolding.} To quickly create basic folder structure and files for new projects in various \acrshort{beam} languages, the \acrshort{cli} provides a convenience command.

\paragraph{Transparent container invocation.} All commands first go through the launcher, which must set up a container to run the given command in; or run the command directly. Many \acrshort{ci} platforms confine the whole build process to a container that has already been set up and started, allowing only limited configuration of the container's parameters. Since it is not always possible to start a clean container, the tool must be able to either operate inside an already-running \acrshort{ci} container, or transparently start a new container and pass control inside.

First, the launcher  attempts to detect where it is being called from by parsing the \emph{control group (cgroup)} file. While detection in this way is widespread, it is not recommended, since it relies on an implementation detail of the container runtime. However, work to add container introspection capabilities is ongoing\footnote{\url{https://github.com/moby/moby/pull/26331}}. To provide a temporary solution until an interface is finalized and widely deployed, the tool checks for artifacts of common container runtime implementations in the \emph{cgroup} file: \emph{Docker}, \emph{\acrfull{lxc}}, and \emph{\acrfull{aws} \acrfull{ecs}}. If the tool is being run inside a container, execution is simply passed to the \emph{container entrypoint} along with all arguments under the assumption that the container's environment is sufficiently correct, i.e.~has the correct version of Erlang/OTP installed.

\paragraph{Determining the base image.} When the launcher determines that it is able to start a container to run the build in, i.e.~the tool is being invoked inside a \acrshort{vm} or on bare metal, it needs to consider the following two attributes to determine which image to instantiate a container from:
\begin{enumerate*}[label=(\roman*)]
  \item The requested version of Erlang/OTP, read from an environment variable, and
  \item the host's machine architecture.
\end{enumerate*}

Concerning the machine architecture, official builds of Erlang/OTP for various architectures are provided on the \emph{Docker Hub}. Such images are available under the namespace of the respective architecture identifiers as used by the \emph{Go} programming language. A challenge lies in reliably normalizing the machine architecture as reported by \lstinline|uname -m|. Note that since version \emph{17.06 Docker} implicitly pulls images for the correct architecture. However, because the tool is designed to support \emph{Docker} down to version \emph{1.12}, the launcher has to manually determine the architecture and map it to an image namespace identifier using the normalization given in table~\ref{table:architectures}.

\begin{table}
  \setlength{\tabcolsep}{10pt}
  \centering
  \begin{tabular}{ r l }
    Output of \lstinline|uname -m| & Identifier \\
    \hline
    \lstinline|arm arm32 armv7 armv7l armhfp| & \lstinline|arm32v7| \\
    \lstinline|arm64 armv8 armv8b armv8l aarch64 aarch64_be| & \lstinline|arm64v8| \\
    \lstinline|i386 i686 i686-64 i686-AT386| & \lstinline|i386| \\
    \lstinline|s390x s390| & \lstinline|s390x| \\
    \lstinline|ppc ppc64 ppcle ppc64le| & \lstinline|ppc64le| \\
    \emph{other} & \lstinline|amd64| \\
  \end{tabular}
  \caption{Mapping between reported machine architecture and image identifier.}\label{table:architectures}
\end{table}

Note that support for other \acrshort{beam} languages such as Elixir is handled within the \emph{container entrypoint}, and it is sufficient to pull an image containing just the Erlang/OTP runtime at this point. Additionally, there are currently no official images of Elixir available for all machine architectures supported by the Erlang/OTP images.


\paragraph{Project mount.} Clearly, the build tool needs access to the directory containing the Erlang/OTP project to be built. The launcher is meant to be invoked inside that directory, and derives the project's name from the name of the current working directory. The build tools must not permanently alter any files of the original project folder on the host as doing so may interfere with other parts of the build pipeline. To guarantee that the project files remain unmodified, the current working directory is mounted as a read-only volume.

\begin{table}[h]
  \setlength{\tabcolsep}{10pt}
  \centering
  \begin{tabular}{ l c l }
    Host & & Container \\
    \hline
    Current working directory &
      $\Longrightarrow$ &
      Project directory \\
    Combined cache directory &
      $\Longleftrightarrow$ &
      $<$\emph{Various cache directories}$>$ \\
    RAM disk or temporary directory &
      $\Longleftrightarrow$ &
      Temporary directory \\
  \end{tabular}
  \caption{Container volume mounts.}
\end{table}

\paragraph{Virtual RAM disk.} The run time of the build tool is bound by performance of the filesystem and storage media. Additionally, building upgrade releases causes the entire project folder to be temporarily duplicated for each previous release.
If supported by the \emph{Docker} client, the container's \lstinline|/tmp| directory is mounted as a temporary filesystem (\emph{tmpfs}) on the host's \acrshort{ram}. In case the size of the temporary file system grows beyond the available \acrshort{ram}, the host \acrshort{os} transparently falls back to consuming swap space.


\paragraph{Cache mount.} Minimizing build run time is crucial for a pleasant \acrlong{ci} workflow. Many \acrshort{ci} platforms offer a way to cache the contents of certain folders between build runs. A number of cache folders of the container are bind-mounted to a single cache path in the host user's home directory. Combining the cache paths into one folder on the host makes it trivial to setup caching, as instead of having to configure multiple, possibly changing paths for each tool separately, it is sufficient to cache just one folder. Currently, the host cache folder combines bind mounts of the following locations inside the container:
\begin{itemize}
  \item Compiled artifacts and dependencies of the build tool itself;
  \item Dependency cache directories of the build tools \lstinline|rebar3| and \lstinline|mix|;
  \item Erlang and Elixir interactive shell history files;
  \item Precompiled, downloaded, and extracted Elixir release.
\end{itemize}
Note that if the launcher is unable to start a container, caching would have to be set up explicitly for each location.

\paragraph{Environment variables.} All configuration for the tool is provided via environment variables, instead of files or interactive menus. Most \acrlong{ci} platforms provide a straightforward way to configure environment variables for the build pipeline, and many offer additional facilities to encrypt or otherwise store environment variables in a secure way. The launcher reads various environment variables and other parameters of the host, applies transformations, and passes configuration on to the \emph{container entrypoint}, again via environment variables.

\begin{table}[h]
  \setlength{\tabcolsep}{10pt}
  \centering
  \begin{tabularx}{\textwidth}{l X X}
    Host & Launcher & Container entrypoint \\
    \hline
    \emph{Working directory} &
      Passed as \lstinline|PROJECT_DIR| \newline
      either as-is or with container path of mount point &
      Passed through \\
    \hline
    \lstinline|ERLANG_VERSION| &
      Used to determine \newline
      image identifier &
      Validated against \newline
      currently installed version \\
    \hline
    \lstinline|ELIXIR_VERSION| &
      Passed through &
      Used to validate and/or \newline
      install Elixir \\
    \hline
    --- &
      \lstinline|TERM| set to \lstinline|dumb| &
      Passed through \\
    \hline
    --- &
      Contents read from global gitignore
      file and passed on as \lstinline|GLOBAL_GITIGNORE| &
      Contents written to \newline
      container's global \newline
      \emph{gitignore} file \\
    \hline
    \lstinline|STORE| & Passed through & Passed through \\
    \hline
    \lstinline|STORE_SECRET| & Passed through & Passed through \\
    \hline
    \lstinline|DEBUG| & Passed through & Passed through \\
  \end{tabularx}
  \caption{Transformation of environment variables between host and container.}\label{table:envvars}
\end{table}

Since the tool must run without interactivity, the launcher exports the \lstinline|TERM| variable set to ``\lstinline|dumb|''. Additionally, the contents of the host's global \emph{gitignore} file are read into an environment variable, passed into the container entrypoint, to be written out again. This avoids situations where host sees the working tree as clean, while the builder refuses to run because files may be present that would be ignored by the host, but not by the builder. Note that neither the architecture of the host, nor the architecture part of the image identifier are passed into the container, as the builder internally retrieves the system architecture string as reported by Erlang/OTP, which differs from the identifier used at the launcher stage. Lastly, authentification credentials for the store are passed through, as is a flag to enable verbose debug output. See table~\ref{table:envvars} for an overview of how various parts of the build pipeline use environment variables to pass and transform configuration.

\paragraph{Container entrypoint.} As stated above, the \acrshort{cli} may be launched inside an already-running container, or start a clean container. In any case, the \emph{container entrypoint} is a \lstinline|bash| script that is always executed inside a container. Yet it may only make minimal assumptions about its environment since the container may have been set up already with parameters beyond the launcher's control. The primary job of the container entrypoint is to compile the builder. It is also responsible for setting up the Erlang code path to include the location of the builder's compiled artifacts, and to add the directory containing the \emph{command scripts} to the system's executable path. Finally, the container entrypoint script calls \lstinline|exec| to replace the execution of itself with the program whose name and arguments were passed through by the \acrshort{cli} launcher. Such constructs are common in container entrypoint scripts, as doing so allows to transparently invoke any executable inside the container as if the program was running on the host, while preparing the environment before passing control onwards.

\paragraph{Command scripts.} Erlang scripts \emph{(escripts)} provide a way to transparently call Erlang code from the system's shell. Some of the tool's commands, including ``build'', ``self test'', and ``install elixir'' are implemented as Erlang scripts. They act as thin wrappers for the Erlang modules that make up the builder application. The responsibility of the \emph{command escripts} is to read from various environment variables and to supply them as valid arguments to the builder application.

\cleardoublepage
\subsection{Builder}

\paragraph{Tool detection.} First, the builder attempts to detect which of the supported build tools are used in the project: Erlang projects are often built with \lstinline|rebar3|, and \lstinline|mix| is mainly used on Elixir projects. Note that some projects may be built with both tools, in which case the builder prefers to invoke \lstinline|mix|.

\paragraph{Sanity checks.} Recall the requirement that a version number of a built release uniquely identifies a committed state of the code base at one point in time. Therefore, the build tool must ensure that the project is in a clean state, i.e.~the state of all tracked files must be equal to that of the last commit, before starting the build process. Note that the working tree may very well contain untracked files that are specific to the currently checked out branch, such as configuration. Since configuration data often includes sensitive credentials, best practices recommend to keep such files out of version control by instructing the \acrshort{vcs} to ignore them. Conversely, the built artifact must include such configuration data. Sanity checks are applied to the repository to make sure that \begin{enumerate*}[label=(\roman*)]
  \item no tracked files have been modified since the last checked out commit; and
  \item that there are no new untracked and un-ignored files present in the working tree.
\end{enumerate*}
Directory layout must follow \acrshort{otp} conventions, and configuration files may not have been moved from their default locations.

\paragraph{Elixir support.}


\paragraph{Compilation.}

\paragraph{Working copy.} Recall the original project folder must not be modified in any way, whether is is mounted as a read-only volume or not. Note that the build tools must be able to freely modify various configuration files of the project, e.g.~overwrite version numbers, add dependencies and output artifacts.
Ideally, the project folder would be mounted as a layered \acrfull{cow} filesystem where the tools may modify any files in a writable upper layer that is overlaid above the original read-only project folder. However, creating overlay filesystem mounts inside a container requires running the container with elevated privileges, which is generally discouraged and often not supported on \acrshort{ci} platforms, as well as requiring additional configuration of the host. Therefore, \acrshort{cow} filesytem mounts are not feasible on \acrshort{ci} platforms. Another way to implement rollback capabilities would be to exploit a \acrfull{vcs} system such as Git to track changes made to the project folder. This approach is not optimal either; as doing so would \begin{enumerate*}[label=(\roman*)]
  \item require to strip existing Git metadata from the project; and
  \item Git is not recommended for tracking changes to large binary files such as compiled artifacts or release tarballs.
\end{enumerate*}

Considering the tool must be trivial to set up on various \acrshort{ci} platforms, a naive solution was chosen: First, mount the project folder as a read-only volume into the container. When various tools need to write to the project, the whole working tree is duplicated --- preferably to a \acrshort{ram} disk, see next paragraph. The tools can then operate on the temporary copies without restriction, and the original project folder on the host cannot be edited by any process inside the container. Note that if the tool is started inside an already-running container, said restriction of read-only volumes does not apply and the tool must take care to never modify the original location by first creating a copy to a temporary location.

\paragraph{Configuration.} To successfully compile releases that can be started on another machine, and may even be hot upgradable, certain configuration parameters are needed for various tools. As the requirements call for minimal setup effort, the developer is not expected to care about these settings or specify them beforehand. First, the release must include a dependency on the \acrfull{sasl} application to have release handling capabilities, i.e.~may be hot upgradable. Next, the Erlang compiler \emph{(erlc)} needs to be instructed to include debug information and abstract code to the compiled \acrshort{beam} files, because application upgrade instructions are generated by inspecting the abstract code. Note that including abstract code in the release makes it possible to reconstruct the source code.

Depending on the build tool used on the project, some configuration files of the project have to be updated. For \lstinline|rebar3| projects, the application upgrade instruction generation plugin is added as a local dependency to the project. Since \lstinline|rebar3| internally uses \lstinline|relx| to assemble the release, the release must be generated with \emph{development mode} disabled, as otherwise dependencies would not be copied into the release but just references via a symbolic link to files inside the ephemeral container, making the produced release unusable.

\paragraph{Auto-versioning.} The version numbers for the applications that make up the release are determined by retrieving last commit that included changes in the respective application's directory. Then the value of the version tuple in each app's configuration file is overwritten with the generated version string. This ensures that the version of an application changes between different releases if and only if the source for that application was changed. Note that minor non-functional changes such as comments or formatting also result in a changed version number.

\paragraph{Building release upgrades.} Recall that assembling a release upgrade used to require the developer to first write \emph{high level} {\acrfull{appup} for each application that is part of the release. Next, \lstinline|systools|, part of \acrshort{otp} \acrfull{sasl}, translate the high level appups into low-level instructions and combine them into a single \acrfull{relup} file. Previous work~\cite{rebar3appup} has shown that the high-level \acrlong{appup} can be generated on a best-effort basis by comparing compiled \acrshort{beam} and application resource files, additionally taking hints from the developer given as appup templates to guide the algorithm in complicated upgrade situations. Consequently, the builder needs access to the previous releases of which the current release will be able to know how to upgrade from.

% Generating upgrade instructions

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{\textwidth}
    \centering
    \begin{tikzpicture}[
      >=latex',
      release/.style={},
      app/.style={rectangle, draw=black!50, anchor=west},
      generator/.style={
        draw,
        shape border rotate=180,
        regular polygon,
        regular polygon sides=3,
        align=center
      }
    ]
      \pgfdeclarelayer{background}
      \pgfdeclarelayer{foreground}
      \pgfsetlayers{background,main,foreground}

      \node [release] (prev1) {Previous Release $n$};
      \node [below=of prev1.west, app, xshift=2mm] (prev1_app1) {App $1$ $\cdot$ version $w$};
      \node [below=of prev1_app1.west, app] (prev1_app2) {App $m$ $\cdot$ version $y$};
      \node [draw=black!50, fit={
        (prev1) (prev1_app1) (prev1_app2)
      }] (prev1_box) {};

      \node [generator, right=of prev1] (appup_generator) {$\lambda$};

      \node [release, right=of appup_generator] (curr1) {Current Release};
      \node [below=of curr1.west, app, xshift=2mm, align=right] (curr1_app1) {App $1$ $\cdot$ version $x$\\$(\star)$ appup $w\Leftrightarrow{}x$};
      \node [below=of curr1_app1.west, app, yshift=-5mm, align=right] (curr1_app2) {App $m$ $\cdot$ version $z$\\$(\star)$ appup $y\Leftrightarrow{}z$};
      \node [draw=black!50, fit={
        (curr1) (curr1_app1) (curr1_app2)
      }] (curr1_box) {};

      \path[draw,->,shorten >=2pt] (prev1 -| prev1_box.east) to (appup_generator);
      \path[draw,->,shorten >=2pt] (curr1 -| curr1_box.west) to (appup_generator);

      \path[draw,->] (appup_generator.south) |- (curr1_app1.189);
      \path[draw,->] (appup_generator.south) |- (curr1_app2.189);

      \begin{pgfonlayer}{background}
        \node [draw=black!50, fit={
          (prev1_box) (curr1_box)
        }, fill=white, double copy shadow={shadow xshift=-4pt,
        shadow yshift=4pt, fill=white, draw}] (generation) {};
      \end{pgfonlayer}

    \end{tikzpicture}
    \subcaption{\acrfull{appup} generation.}
  \end{subfigure}
  \vspace{10pt} \\
  \begin{subfigure}[b]{\textwidth}
    \centering
    \begin{tikzpicture}[
      >=latex',
      release/.style={},
      app/.style={rectangle, draw=black!50, anchor=west},
      generator/.style={
        draw,
        shape border rotate=180,
        regular polygon,
        regular polygon sides=3,
        align=center
      }
    ]
      \pgfdeclarelayer{background}
      \pgfdeclarelayer{foreground}
      \pgfsetlayers{background,main,foreground}

      \node [release] (prev1) {Previous Release $n$};
      \node [below=of prev1.west, app, xshift=2mm] (prev1_app1) {App $1$ $\cdot$ version $w$};
      \node [below=of prev1_app1.west, app] (prev1_app2) {App $m$ $\cdot$ version $y$};
      \node [draw=black!50, fit={
        (prev1) (prev1_app1) (prev1_app2)
      }] (prev1_box) {};

      \node [generator, right=of prev1] (appup_generator) {$\lambda$};

      \node [release, right=of appup_generator] (curr1) {Current Release};
      \node [below=of curr1.west, app, xshift=2mm, align=right] (curr1_app1) {App $1$ $\cdot$ version $x$\\appup $w\Leftrightarrow{}x$};
      \node [below=of curr1_app1.west, app, yshift=-5mm, align=right] (curr1_app2) {App $m$ $\cdot$ version $z$\\appup $y\Leftrightarrow{}z$};
      \node [below=of curr1_app2.west, app, yshift=-4mm] (relup) {$(\star)$ relup};
      \node [draw=black!50, fit={
        (curr1) (curr1_app1) (curr1_app2) (relup)
      }] (curr1_box) {};

      \path[draw,->,shorten >=2pt] (prev1 -| prev1_box.east) to (appup_generator);
      \path[draw,->,shorten >=2pt] (curr1 -| curr1_box.west) to (appup_generator);


      \path[draw,->,shorten >=5pt] (curr1_app1.189) -| (appup_generator.south east);
      \path[draw,->,shorten >=5pt] (curr1_app2.189) -| (appup_generator.south east);

      \path[draw,->] (appup_generator.south) |- (relup);

      \begin{pgfonlayer}{background}
        \node [draw=black!50, fit={
          (prev1_box) (curr1_box)
        }, fill=white, double copy shadow={shadow xshift=-4pt,
        shadow yshift=4pt, fill=white, draw}] (generation) {};
      \end{pgfonlayer}

    \end{tikzpicture}
    \subcaption{\acrfull{relup} generation.\\
    \vspace{10pt} $(\star)$ denotes new files.}
  \end{subfigure}
  \caption{Generating upgrade instructions.}
\end{figure}

\begin{table}[h]
  \setlength{\tabcolsep}{2pt}
  \renewcommand{\arraystretch}{1.5}
  \centering
  \begin{tabular}{ r r c r r }
    n: & $w\;\Leftrightarrow\;{}x$ &
      \multirow{4}{*}{
        \hspace{15pt}
        \Large{$\succ$}
        \hspace{15pt}
      }& \\
    n: & $g\;\Leftrightarrow\;{}x$ & &
      n: & $(w|g)\;\Leftrightarrow\;{}x$ \\
    $m$: & $y\;\Leftrightarrow\;{}z$ & &
      m: & $(y|h)\;\Leftrightarrow\;{}z$ \\
    $m$: & $h\;\Leftrightarrow\;{}z$ & & \\
  \end{tabular}
  \caption{Combining collected upgrade instructions.}\label{table:architectures}
\end{table}


\paragraph{Fetching previous releases.} While the project is being compiled, the builder queries the store for a list of the version identifiers of all currently stored artifacts that match the name of the project being built, and had been compiled on the same machine architecture the builder is currently running on. The builder then filters this list to exclude the current combination of version identifier and branch. Note that the branch identifier is only considered for excluding the current combination, but not for other versions, as it might be helpful to generate release upgrades between branches, so that a node may hot-switch its branch by applying a cross-branch upgrade. Each of the remaining releases are then downloaded and extracted to a temporary location.

\paragraph{Generating \acrshort{appup}s.} For Erlang projects built with \lstinline|rebar3|, the \acrlong{appup} generator plugin~\cite{rebar3appup} is used. A limitation of this plugin is that it can only generate \acrshort{appup}s for the applications between exactly two releases; while the \acrshort{appup} specification includes the possibility of a single \acrshort{appup} file containing instructions to upgrade from multiple previous versions. Even when the \acrshort{appup} generator is invoked multiple times, it overwrites the previously generated file instead of appending to the instructions. Hence, the \acrshort{appup} generator is invoked multiple times, each iteration between a single previous release and a fresh working copy of the current project. Each time after the generator plugin was run, the resulting \acrshort{appup} files are collected from the application subdirectories and their contents and relative path are saved in a temporary structure. After \acrlong{appup} had been generated and collected for all applications of all previous releases, they are grouped by the application which they belong to. For each application the collected instructions are merged into a single \acrshort{appup} file that is written back to the relative location of the \acrshort{appup} file in the respective application of the current project.

\paragraph{Generating relups.} Erlang/\acrshort{otp} ships with the \acrfull{sasl} application that includes a function to assemble multiple high-level \acrfull{appup} files into a single low-level \acrfull{relup} file. Additionally, \lstinline|rebar3| wraps the \acrshort{relup} assembler in a convenience command that takes care to correctly set up the environment before invoking the generator. While the bare \acrshort{relup} generator handles multiple previous releases directly, the wrapper provided by \lstinline|rebar3| is in fact another wrapper over \lstinline|relx|, which finally calls \acrshort{sasl}. At one point in this chain, the ability to handle multiple previous releases is lost, so the tool has to engage a similar technique as the one used for generating \acrshort{appup}s. For each previous release, the current project is cloned, because in addition to the \acrshort{appup} files, the \acrshort{relup} generator also needs access to the previous release resource file and expects previous versions of the applications inside the application directory of the current release, so the previous applications are copied to the clone of the current project. The \acrshort{relup} generator is invoked to produce a \lstinline|relup| file inside the clone of the current project. The resulting \acrlong{relup} are again collected and the temporary clones are deleted. In contrast to the \acrshort{appup} generation process, there is now only a single \acrshort{relup} file for each previous release. These files have almost the same structure as the \acrshort{appup} files so the the tool can merge the collected instructions in the same way, and write the resulting \acrshort{relup} file to the current release.

\paragraph{Packaging and storing the release.} With a final, merged \acrfull{relup} file in place, the release is ready to be packaged as a gzip-compressed tarball (\lstinline|*.tar.gz|), which contains everything needed to start a \emph{target system} from scratch, or to hot-upgrade a system running any of the versions of any branches that were present in the store when the build process was started. The resulting tarball artifact is sent to the store.

\begin{figure}[h]
  \centering
  \vspace{5mm}
  \begin{forest}
    dir tree
    [./$<$\emph{project name}$>$-$<$\emph{release version}$>$.tar.gz
      [bin]
      [erts-$<$\emph{erts version}$>$\quad\ldots]
      [lib
        [$<$\emph{application 1 name}$>$-$<$\emph{app.~1 version}$>$
          [ebin
            [$<$\emph{application name}$>$.app]
            [$<$\emph{module 1\ldots{}m}$>$.beam]
          ]
          [include
            [$<$\emph{header 1\ldots{}m}$>$.hrl]
          ]
        ]
        [$<$\emph{application n name}$>$-$<$\emph{app.~n version}$>$
          [ebin\quad\ldots]
          [include\quad\ldots]
        ]
        [kernel-$<$\emph{kernel version}$>$\quad\ldots]
        [sasl-$<$\emph{sasl version}$>$\quad\ldots]
        [stdlib-$<$\emph{stdlib version}$>$\quad\ldots]
      ]
      [releases
        [$<$\emph{release version}$>$
          [$<$\emph{project name}$>$.rel]
          [relup]
          [start.boot]
          [sys.config]
          [vm.args]
        ]
        [$<$\emph{project name}$>$.rel]
        [RELEASES]
        [start\_erl.data]
      ]
    ]
  \end{forest}
  \caption{Directory tree of an \acrshort{otp} Release artifact.}
\end{figure}

\cleardoublepage
\subsection{Store}

\paragraph{\acrshort{api}.}
